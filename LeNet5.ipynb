{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciamartinf/BigData/blob/main/LeNet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LeNet5 Model with BigDL\n",
        "\n",
        "#### Ángela Gómez Sacristán, Álvaro González Berdasco y Lucía Martín Fernández"
      ],
      "metadata": {
        "id": "GdgPsMzXrqxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BigDL environment set-up"
      ],
      "metadata": {
        "id": "wUYDthPGtTHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BigDL Dllib installation"
      ],
      "metadata": {
        "id": "cSLfe5hzwV4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install latest pre-release version of bigdl-dllib with spark3\n",
        "# Find the latest bigdl-dllib with spark3 from https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/ and intall it\n",
        "!pip install https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/bigdl_dllib_spark3-0.14.0b20211107-py3-none-manylinux1_x86_64.whl\n",
        "\n",
        "#exit() # restart the runtime to refresh installed pkg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_666G3J4lc5w",
        "outputId": "dc859d6e-3273-4719-97bf-e46ecdcbbdbe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bigdl-dllib-spark3==0.14.0b20211107\n",
            "  Downloading https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/bigdl_dllib_spark3-0.14.0b20211107-py3-none-manylinux1_x86_64.whl (93.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-dllib-spark3==0.14.0b20211107) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.8/dist-packages (from bigdl-dllib-spark3==0.14.0b20211107) (1.21.6)\n",
            "Collecting pyspark==3.1.2\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880769 sha256=598566f578fce705f6d0e60568fa7566997461c108c2c3e43bffb04076f2fd9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/88/9e/58ef1f74892fef590330ca0830b5b6d995ba29b44f977b3926\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, bigdl-dllib-spark3\n",
            "Successfully installed bigdl-dllib-spark3-0.14.0b20211107 py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "CcnmozzgbobP",
        "outputId": "d61c16c0-d356-4569-ffc0-aff9fe66a7a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing principal libraries"
      ],
      "metadata": {
        "id": "NoO3HDA_wFN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "import datetime as dt\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os, random\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "from bigdl.dllib.nncontext import *\n",
        "from bigdl.dllib.nnframes import *\n",
        "from bigdl.dllib.nn.criterion import *\n",
        "from bigdl.dllib.nn.layer import *\n",
        "from bigdl.dllib.optim.optimizer import *\n",
        "\n",
        "from bigdl.dllib import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import load_img\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image"
      ],
      "metadata": {
        "id": "vNt0LWVulZks",
        "outputId": "096b0bca-6222-45a2-a03f-4d6871771e5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepending /usr/local/lib/python3.8/dist-packages/bigdl/share/dllib/conf/spark-bigdl.conf to sys.path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting-up Spark Session"
      ],
      "metadata": {
        "id": "37uDsC13xSie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "5NNIFRU1buFN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = init_nncontext(cluster_mode=\"local\") # run in local mode\n",
        "spark = SparkSession(sc)\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Foo\") \\\n",
        "    .config(\"spark.executor.memory\", '50G') \\\n",
        "    .config(\"spark.driver.memory\", '50G') \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "6dr-aMyymFnv",
        "outputId": "3de10aef-f67f-4471-caba-f48f4bf32f8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current pyspark location is : /usr/local/lib/python3.8/dist-packages/pyspark/__init__.py\n",
            "Start to getOrCreate SparkContext\n",
            "pyspark_submit_args is:  --driver-class-path /usr/local/lib/python3.8/dist-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_3.1.2-0.14.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n",
            "Successfully got a SparkContext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "qjzbgsEbxZHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "prbSzq0Amoea",
        "outputId": "8adfb5b8-3070-47b4-8e01-69adf7a7c561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to the data directory\n",
        "dir_alldata = Path('/content/drive/MyDrive/Colab Notebooks/chest_xray')\n",
        "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
        "train_data_dir = dir_alldata / 'train'\n",
        "\n",
        "# Path to validation directory\n",
        "validation_data_dir = dir_alldata / 'val'\n",
        "\n",
        "# Path to test directory\n",
        "test_data_dir = dir_alldata / 'test'\n",
        "\n",
        "# Get the path to the normal and pneumonia sub-directories\n",
        "normal_cases_train = train_data_dir / 'NORMAL'\n",
        "pneumonia_cases_train = train_data_dir / 'PNEUMONIA'"
      ],
      "metadata": {
        "id": "ujg74xgAmpE8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data transformation\n",
        "\n",
        "Transformation of the images to generators and then to np.arrays, together with its correspondent labels"
      ],
      "metadata": {
        "id": "V-aKhKtHxxG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 32,32 \n",
        "nb_train_sample = 5216\n",
        "nb_validation_samples =16\n",
        "nb_test_samples = 624\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "if K.image_data_format()==\"channels_first\":\n",
        "    input_shape =(3,img_width, img_height)\n",
        "else:\n",
        "    input_shape =(img_width, img_height,3)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator= train_datagen.flow_from_directory(train_data_dir, target_size =(150,150), batch_size = batch_size, class_mode=\"binary\" )\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_data_dir, target_size = (150,150), batch_size =  batch_size, class_mode=\"binary\")\n",
        "test_generator = test_datagen.flow_from_directory(test_data_dir, target_size = (150,150), batch_size= batch_size,  class_mode=\"binary\")"
      ],
      "metadata": {
        "id": "exbq13kimssd",
        "outputId": "7a01c5ec-3a59-4032-a95b-65fd813859a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generator datatypes automatically generate data batches. However to obtain the full data in np.array mode we used the tqdm library:"
      ],
      "metadata": {
        "id": "ZazQdf6N0Foo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tqdm  # if not install\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "WcuRNR-MMGb5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the train dataset\n",
        "train_generator.reset()\n",
        "X_train, y_train = next(train_generator)\n",
        "for i in tqdm.tqdm(range(int(train_generator.n/batch_size)-1)): \n",
        "  img, label = next(train_generator)\n",
        "  X_train = np.append(X_train, img, axis=0 )\n",
        "  y_train = np.append(y_train, label, axis=0)\n"
      ],
      "metadata": {
        "id": "Eon7ZqA0QbBa",
        "outputId": "2bd50443-8715-45bf-e125-851301f404ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 162/162 [02:26<00:00,  1.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "lTYmg3M-ZMhz",
        "outputId": "7ae53687-723f-43f7-a2ff-3e1467191b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5216, 150, 150, 3)\n",
            "(5216,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the test dataset\n",
        "test_generator.reset()\n",
        "X_test, Y_test = next(test_generator)\n",
        "for i in tqdm.tqdm(range(int(test_generator.n/batch_size)-1)): \n",
        "  img, label = next(test_generator)\n",
        "  X_test = np.append(X_test, img, axis=0 )\n",
        "  Y_test = np.append(Y_test, label, axis=0)"
      ],
      "metadata": {
        "id": "4fXVZflGb4xV",
        "outputId": "9a91b559-66de-4362-c295-94eebb358986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:06<00:00,  2.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since validation size is smaller than the batch size we don't need to perform the tqdm step.\n",
        "X_val, Y_val = next(validation_generator)"
      ],
      "metadata": {
        "id": "paiQ6f_62T9c"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeNet5 Model"
      ],
      "metadata": {
        "id": "gF78fkrW2jE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ],
      "metadata": {
        "id": "iANN3Db83Y8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lenet_model():\n",
        "\n",
        "    \"\"\"\n",
        "    Function to build the lenet model as Sequential datatype\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Reshape((3, 150, 150), input_shape=(150, 150, 3)))\n",
        "    model.add(Convolution2D(6, 5, 5, activation=\"tanh\", name=\"conv1_5x5\"))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Convolution2D(12, 5, 5, activation=\"tanh\", name=\"conv2_5x5\"))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation=\"tanh\", name=\"fc1\"))\n",
        "    model.add(Dense(2, activation=\"softmax\", name=\"fc2\")) # 2 classes\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "62Ojsv4apjDJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet = build_lenet_model()"
      ],
      "metadata": {
        "id": "dIuynAVbplzx",
        "outputId": "70827ea1-86e0-4f66-8d85-1a936ad8d153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createZooKerasSequential\n",
            "creating: createZooKerasReshape\n",
            "creating: createZooKerasConvolution2D\n",
            "creating: createZooKerasMaxPooling2D\n",
            "creating: createZooKerasConvolution2D\n",
            "creating: createZooKerasMaxPooling2D\n",
            "creating: createZooKerasFlatten\n",
            "creating: createZooKerasDense\n",
            "creating: createZooKerasDense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Compilation"
      ],
      "metadata": {
        "id": "8Ri5yLps3biO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we need to compile the model so the model is optimized in every training step. "
      ],
      "metadata": {
        "id": "b7AD6_d42_zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adadelta',\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "X2WsuuFcp-gP",
        "outputId": "02c3ec52-ecb3-4643-f8f9-fe660fa67aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createAdadelta\n",
            "creating: createZooKerasSparseCategoricalCrossEntropy\n",
            "creating: createZooKerasSparseCategoricalAccuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model fitting"
      ],
      "metadata": {
        "id": "OAbadu1D3fMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were not able to fit the model with the whole dataset so we selected a smallest portion of the data:"
      ],
      "metadata": {
        "id": "io5U_80B3Naj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sX_train = X_train[:500]\n",
        "sy_train = y_train[:500]"
      ],
      "metadata": {
        "id": "UuIth8O7B7bw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet.fit(\n",
        "        sX_train, sy_train,\n",
        "        nb_epoch = 200,\n",
        "        batch_size = 50,\n",
        "        validation_data=(X_val, Y_val))"
      ],
      "metadata": {
        "id": "2pGOP1UiCGYv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the fit_generator option, we could have directly use the train_generator object as input and our fitting could have much better. However, this function is not implemented in the bigdl.dllib library "
      ],
      "metadata": {
        "id": "T_oXq2hE3hYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ],
      "metadata": {
        "id": "tjlmoYaw32fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = lenet.evaluate(X_test, Y_test)\n",
        "print(\"TestLoss: \", accuracy[0])\n",
        "print(\"Accuracy: \", accuracy[1])"
      ],
      "metadata": {
        "id": "5X63LEKB9zUt",
        "outputId": "5336f601-1b66-43c4-837c-e9be9c1f3d85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TestLoss:  2.4382448196411133\n",
            "Accuracy:  0.7713815569877625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the model can still improve a lot since we are obtained a pretty high Loss. However, the accuracy obtained is not so bad. This shows the potential of this model, with further training and using more data, we could expect some promising results"
      ],
      "metadata": {
        "id": "bzvVs5Pq6Vfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments"
      ],
      "metadata": {
        "id": "fw2z-PQt39PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found out that functions and methods implemented in the bigdl.dllib library are sometimes pretty different from original keras methods and functions which complicated the building of different models implement the bigdl.dllib library. \n",
        "\n",
        "\n",
        "Considering these difficulties, we decided to build the lenet model implement the bigdl.dllib library and two other different models using tensorflow "
      ],
      "metadata": {
        "id": "rkE1It0YMj-M"
      }
    }
  ]
}